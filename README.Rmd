---
title: "sparkreadr"
author: "Daniel Emaasit"
date: "`r Sys.Date()`"
output: md_document
---

```{r setup, include=FALSE, eval = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Read flat files into Spark DataFrames

## What is sparkreadr?

sparkreadr is an extension for sparklyr to read flat files into Spark DataFrames. It uses the Spark package spark-csv.

## Installation
sparkreadr requires the sparklyr package to run

### Install sparklyr
I recommend the latest stable version of sparklyr available on CRAN
```{r eval = FALSE}
install.packages("sparklyr")
```

### Install sparkreadr
Install the development version of sparkreadr from this Github repo using devtools
```{r eval = FALSE}
library(devtools)
devtools::install_github("emaasit/sparkreadr")
```

## Connecting to Spark

If Spark is not already installed, use the following sparklyr command to install your preferred version of Spark:
```{r eval = FALSE}
library(sparklyr)
spark_install(version = "2.0.0")
```

The call to `library(sparkreadr)` will make the sparkreadr functions available on the R search path and will also ensure that the dependencies required by the package are included when we connect to Spark.
```{r eval = FALSE}
library(sparkreadr) 
```

We can create a Spark connection as follows:
```{r eval = FALSE}
sc <- spark_connect(master = "local")
```

## Reading readr files

sparkreadr provides the function `spark_read_readr` to read readr data files into Spark DataFrames. It uses a Spark package called spark-readr. Here's an example.

```{r eval = FALSE}
mtcars_file <- system.file("extdata", "mtcars.csv", package = "sparkreadr")

mtcars_df <- spark_read_csv(sc, path = mtcars_file, table = "readr_table")
mtcars_df
```

The resulting pointer to a Spark table can be further used in dplyr statements.
```{r eval = FALSE}
library(dplyr)
mtcars_df %>% group_by(cyl) %>%
  summarise(count = n(), avg.mpg = mean(mpg), avg.displacment = mean(disp), avg.horsepower = mean(hp))
```

## Logs & Disconnect

Look at the Spark log from R:
```{r eval = FALSE}
spark_log(sc, n = 100)
```

Now we disconnect from Spark:
```{r eval = FALSE}
spark_disconnect(sc)
```

## Acknowledgements
Thanks to RStudio for the sparklyr package that provides functionality to create Extensions.
